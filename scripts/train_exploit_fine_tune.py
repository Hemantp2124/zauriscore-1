"""
Fine-tune CodeBERT/GraphCodeBERT on exploit datasets for ZaurisCore.
This script loads preprocessed SmartBugs data, tokenizes Solidity code,
fine-tunes models for vulnerability classification, and saves checkpoints.
Requirements: transformers, torch, datasets, scikit-learn, mlflow
Run after fetching datasets with fetch_exploit_datasets.py
"""

import os
import sys
import json
import hashlib
from pathlib import Path

# Add project root to Python path
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import torch
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding
)
from transformers.modeling_outputs import SequenceClassifierOutput
from datasets import Dataset

import mlflow
from mlflow.tracking import MlflowClient

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_squared_error
import numpy as np
import glob

import tempfile
from zauriscore.analyzers.cfg_taint_analyzer import CFGTAintAnalyzer

# Constants
MODEL_NAME = 'microsoft/codebert-base'  # Or 'microsoft/graphcodebert-base' for graph-aware
OUTPUT_DIR = Path('src/zauriscore/models/checkpoints/exploit_fine_tuned')
# Use local file-based MLflow tracking
MLFLOW_TRACKING_URI = './mlruns'  # Local directory for MLflow tracking

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
Path(MLFLOW_TRACKING_URI).mkdir(parents=True, exist_ok=True)


def load_dataset():
    # Load VulDee dataset
    vuldee_dir = Path('data/exploit_datasets/processed/vuldee')
    if not vuldee_dir.exists():
        raise FileNotFoundError(f'VulDee dataset not found at {vuldee_dir}. Run fetch_exploit_datasets.py first.')
    
    print(f"Loading dataset from {vuldee_dir}...")
    
    # List all Solidity files
    sol_files = list(vuldee_dir.glob('*.sol'))
    if not sol_files:
        raise ValueError(f'No Solidity files found in {vuldee_dir}')
    
    print(f"Found {len(sol_files)} Solidity files")
    
    # Version dataset
    content_hash = hashlib.sha256(''.join([d['code'] for d in all_data]).encode()).hexdigest()[:16]
    print(f'Combined dataset loaded: {len(all_data)} samples ({len(data_smartbugs)} SmartBugs + {len(data_vuldee)} VulDee), version {content_hash}')
    
    # Split: 80/20, stratify on labels
    train_data, test_data = train_test_split(all_data, test_size=0.2, stratify=[d['label'] for d in all_data], random_state=42)
    
    return {'train': train_data, 'test': test_data, 'feature_names': feature_names}


def extract_numeric_features(contract_code):
    metrics = {
        'function_count': len(contract_code.split('function')),
        'security_flags': 1 if 'revert' in contract_code.lower() else 0,
        'complexity': len(contract_code.split('if'))
    }
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.sol', delete=False) as tmp_file:
            tmp_file.write(contract_code)
            tmp_path = tmp_file.name
        analyzer = CFGTAintAnalyzer(tmp_path)
        analyzer.extract_cfg()
        analyzer.perform_taint_analysis()
        ml_features = analyzer.generate_features_for_ml()
        agg_features = {}
        for func_features in ml_features.values():
            for k, v in func_features.items():
                if isinstance(v, (int, float)):
                    agg_features[f'{k}_avg'] = agg_features.get(f'{k}_avg', 0) + v
                elif isinstance(v, list):
                    agg_features['num_taint_paths'] = agg_features.get('num_taint_paths', 0) + len(v)
        num_funcs = len(ml_features)
        if num_funcs > 0:
            for k in ['num_nodes', 'num_edges', 'avg_degree', 'density', 'longest_path', 'num_tainted_ops']:
                full_k = f'{k}_avg'
                if full_k in agg_features:
                    agg_features[k] = agg_features[full_k] / num_funcs
                    del agg_features[full_k]
            if 'num_taint_paths' in agg_features:
                agg_features['num_taint_paths_avg'] = agg_features['num_taint_paths'] / num_funcs
                del agg_features['num_taint_paths']
            agg_features['taint_ratio'] = agg_features.get('num_tainted_ops', 0) / max(1, metrics['function_count'])
        metrics.update(agg_features)
    except Exception as e:
        print(f"CFG/Taint extraction failed: {e}")
    finally:
        if tmp_path:
            os.unlink(tmp_path)
    feature_names = sorted(metrics.keys())
    numeric = [metrics[k] for k in feature_names]
    return numeric, feature_names

def tokenize_function(examples, tokenizer):
    tokenized = tokenizer(
        examples['code'],
        truncation=True,
        padding=False,
        max_length=512,
    )
    tokenized['numeric_features'] = examples['numeric_features']
    tokenized['labels'] = examples['label']
    return tokenized


def compute_metrics(eval_pred):
    """Compute F1, precision, recall, accuracy for classification."""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')
    acc = accuracy_score(labels, predictions)
    
    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}


class CustomDataCollator:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, features):
        # Pad tokenized inputs
        input_features = [{'input_ids': f['input_ids'], 'attention_mask': f['attention_mask']} for f in features]
        batch = self.tokenizer.pad(input_features, padding=True, return_tensors='pt')
        # Labels
        batch['labels'] = torch.tensor([f['labels'] for f in features], dtype=torch.long)
        # Numeric features
        batch['numeric_features'] = torch.tensor([f['numeric_features'] for f in features], dtype=torch.float)
        return batch


class CustomExploitClassifier(torch.nn.Module):
    def __init__(self, model_name, num_labels, num_numeric_features):
        super().__init__()
        self.base_model = AutoModel.from_pretrained(model_name)
        self.hidden_size = self.base_model.config.hidden_size
        self.num_labels = num_labels
        self.numeric_linear = torch.nn.Linear(num_numeric_features, self.hidden_size)
        self.dropout = torch.nn.Dropout(self.base_model.config.hidden_dropout_prob)
        self.classifier = torch.nn.Linear(self.hidden_size * 2, num_labels)

    def forward(self, input_ids=None, attention_mask=None, numeric_features=None, labels=None, **kwargs):
        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)
        pooled_output = outputs.pooler_output
        numeric_hidden = self.numeric_linear(numeric_features.float())
        concat_hidden = torch.cat([pooled_output, numeric_hidden], dim=-1)
        concat_hidden = self.dropout(concat_hidden)
        logits = self.classifier(concat_hidden)
        loss = None
        if labels is not None:
            loss_fct = torch.nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}


def main():
    # Setup MLflow with local file-based tracking
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    
    try:
        mlflow.set_experiment('zauriscore_exploit_fine_tune')
    except Exception as e:
        print(f"Warning: Could not set MLflow experiment: {e}")
        print("Continuing with default experiment...")
    
    with mlflow.start_run() as run:
        # Load data
        datasets = load_dataset()
        feature_names = datasets.pop('feature_names')
        total_size = len(datasets['train']) + len(datasets['test'])
        mlflow.log_param('total_samples', total_size)
        numeric_dim = len(feature_names)
        
        # Tokenizer and model
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        model = CustomExploitClassifier(MODEL_NAME, num_labels=2, num_numeric_features=numeric_dim)
        
        # Tokenize
        train_enc = tokenize_function({
            'code': [d['code'] for d in datasets['train']],
            'label': [d['label'] for d in datasets['train']],
            'numeric_features': [d['numeric_features'] for d in datasets['train']]
        }, tokenizer)
        test_enc = tokenize_function({
            'code': [d['code'] for d in datasets['test']],
            'label': [d['label'] for d in datasets['test']],
            'numeric_features': [d['numeric_features'] for d in datasets['test']]
        }, tokenizer)
        
        train_dataset = Dataset.from_dict(train_enc)
        test_dataset = Dataset.from_dict(test_enc)
        
        data_collator = CustomDataCollator(tokenizer)
        
        # Training args
        training_args = TrainingArguments(
            output_dir=OUTPUT_DIR,
            num_train_epochs=3,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            warmup_steps=500,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=10,
            evaluation_strategy='epoch',
            save_strategy='epoch',
            load_best_model_at_end=True,
            metric_for_best_model='f1',
            greater_is_better=True,
            report_to='mlflow',
            run_name='codebert_exploit_ft'
        )
        
        # Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=compute_metrics
        )
        
        # Train
        trainer.train()
        
        # Evaluate
        eval_results = trainer.evaluate()
        print(f'Eval results: {eval_results}')
        
        # Log to MLflow
        mlflow.log_metrics(eval_results)
        mlflow.log_param('model_name', MODEL_NAME)
        mlflow.log_param('epochs', 3)
        mlflow.log_param('batch_size', 16)
        
        # Save model
        trainer.save_model(OUTPUT_DIR / 'final_model')
        tokenizer.save_pretrained(OUTPUT_DIR / 'final_model')
        
        print(f'Model saved to {OUTPUT_DIR / "final_model"}')
        print('Fine-tuning complete! Target F1 >= 0.85 achieved? Check MLflow UI.')


if __name__ == '__main__':
    # Check GPU
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f'Using device: {device}')
    main()