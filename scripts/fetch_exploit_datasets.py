"""Script to fetch and preprocess exploit datasets for ZauriScore ML training.

Fetches SmartBugs dataset (Solidity vulnerabilities) from GitHub.
Preprocesses for CodeBERT/GraphCodeBERT fine-tuning.
"""

import os
import json
import requests
from pathlib import Path
from zipfile import ZipFile
import hashlib

# Configuration
SMARTBUGS_REPO = "https://github.com/smartbugs/smartbugs-dataset/archive/refs/heads/master.zip"
OUTPUT_DIR = Path("data/exploit_datasets")
SMARTBUGS_DIR = OUTPUT_DIR / "smartbugs"


def fetch_smartbugs_dataset():
    """Download and extract SmartBugs dataset."""
    if not SMARTBUGS_DIR.exists():
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        print("Downloading SmartBugs dataset...")
        response = requests.get(SMARTBUGS_REPO)
        response.raise_for_status()
        
        zip_path = OUTPUT_DIR / "smartbugs.zip"
        with open(zip_path, 'wb') as f:
            f.write(response.content)
        
        with ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(OUTPUT_DIR)
        
        # Rename extracted folder
        extracted_dir = OUTPUT_DIR / "smartbugs-dataset-master"
        extracted_dir.rename(SMARTBUGS_DIR)
        
        os.remove(zip_path)
        print(f"SmartBugs dataset extracted to {SMARTBUGS_DIR}")
    else:
        print("SmartBugs dataset already exists.")


def compute_dataset_hash(directory: Path) -> str:
    """Compute SHA256 hash of dataset for versioning."""
    hasher = hashlib.sha256()
    for root, _, files in os.walk(directory):
        for file in sorted(files):
            if file.endswith('.sol'):
                filepath = Path(root) / file
                with open(filepath, 'rb') as f:
                    hasher.update(f.read())
    return hasher.hexdigest()


def preprocess_dataset():
    """Preprocess dataset: label vulnerabilities, normalize code."""
    processed_dir = OUTPUT_DIR / "processed"
    processed_dir.mkdir(exist_ok=True)
    
    # SmartBugs provides JSON reports with findings
    results_dir = SMARTBUGS_DIR / "results"
    if results_dir.exists():
        for result_file in results_dir.glob("*.json"):
            with open(result_file, 'r') as f:
                report = json.load(f)
            
            contract_name = result_file.stem
            source_file = SMARTBUGS_DIR / "bytecode" / f"{contract_name}.sol"  # Adjust path as needed
            
            if source_file.exists():
                with open(source_file, 'r') as f:
                    source_code = f.read()
                
                # Label: 1 if vulnerabilities found, 0 otherwise
                label = 1 if report.get('vulnerabilities', []) else 0
                
                entry = {
                    'source_code': source_code,
                    'label': label,
                    'vulnerabilities': report.get('vulnerabilities', []),
                    'cutoff_date': '2023-01-01'  # Update with actual fetch date
                }
                
                output_file = processed_dir / f"{contract_name}.json"
                with open(output_file, 'w') as f:
                    json.dump(entry, f, indent=2)
    
    # Compute hash for versioning
    dataset_hash = compute_dataset_hash(processed_dir)
    with open(OUTPUT_DIR / "dataset_version.txt", 'w') as f:
        f.write(f"Hash: {dataset_hash}\nDate: {json.dumps({'fetch_date': '2023-10-01'})}")
    
    print(f"Processed dataset saved to {processed_dir}")
    print(f"Version hash: {dataset_hash[:16]}...")


if __name__ == "__main__":
    fetch_smartbugs_dataset()
    preprocess_dataset()
    print("Dataset preparation complete. Ready for ML fine-tuning.")