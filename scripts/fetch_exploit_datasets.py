"""Script to fetch and preprocess exploit datasets for ZauriScore ML training.

Fetches SmartBugs dataset (Solidity vulnerabilities) from GitHub.
Preprocesses for CodeBERT/GraphCodeBERT fine-tuning.
"""

import os
import json
import requests
from pathlib import Path
from zipfile import ZipFile
import hashlib
import re  # Add for potential parsing helpers if needed, but may not be necessary

# Configuration
SMARTBUGS_REPO = "https://github.com/smartbugs/smartbugs-wild/archive/refs/heads/master.zip"
OUTPUT_DIR = Path("data/exploit_datasets")
SMARTBUGS_DIR = OUTPUT_DIR / "smartbugs"
import sys

script_path = Path(__file__).parent
project_root = script_path.parent
VULDEE_DATA_PATH = project_root / "VulDeeSmartContract/data/SmartContract.txt"
VULDEE_PROCESSED_DIR = OUTPUT_DIR / "processed" / "vuldee"


def fetch_smartbugs_dataset():
    """Download and extract SmartBugs dataset."""
    try:
        if not SMARTBUGS_DIR.exists():
            OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
            
            print("Downloading SmartBugs dataset...")
            response = requests.get(SMARTBUGS_REPO, stream=True)
            response.raise_for_status()
            
            zip_path = OUTPUT_DIR / "smartbugs.zip"
            with open(zip_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            print("Extracting SmartBugs dataset...")
            with ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(OUTPUT_DIR)
            
            # Rename extracted folder
            extracted_dir = OUTPUT_DIR / "smartbugs-wild-master"
            if extracted_dir.exists():
                extracted_dir.rename(SMARTBUGS_DIR)
                print(f"SmartBugs dataset extracted to {SMARTBUGS_DIR}")
            else:
                print(f"Warning: Expected directory not found: {extracted_dir}")
                # Check what was actually extracted
                print("Contents of output directory:")
                for item in OUTPUT_DIR.iterdir():
                    print(f"  - {item.name}")
            
            if zip_path.exists():
                os.remove(zip_path)
        else:
            print("SmartBugs dataset already exists.")
    except Exception as e:
        print(f"Error downloading or extracting SmartBugs dataset: {e}")
        print(f"Please check your internet connection and try again.")
        print(f"You can also manually download the dataset from: {SMARTBUGS_REPO}")
        print(f"and extract it to: {SMARTBUGS_DIR}")


def compute_dataset_hash(directory: Path) -> str:
    """Compute SHA256 hash of dataset for versioning."""
    hasher = hashlib.sha256()
    for root, _, files in os.walk(directory):
        for file in sorted(files):
            if file.endswith('.sol'):
                filepath = Path(root) / file
                with open(filepath, 'rb') as f:
                    hasher.update(f.read())
    return hasher.hexdigest()


def preprocess_dataset():
    """Preprocess dataset: label vulnerabilities, normalize code."""
    processed_dir = OUTPUT_DIR / "processed"
    processed_dir.mkdir(exist_ok=True)
    
    # SmartBugs provides JSON reports with findings
    results_dir = SMARTBUGS_DIR / "results"
    if results_dir.exists():
        for result_file in results_dir.glob("*.json"):
            with open(result_file, 'r') as f:
                report = json.load(f)
            
            contract_name = result_file.stem
            source_file = SMARTBUGS_DIR / "bytecode" / f"{contract_name}.sol"  # Adjust path as needed
            
            if source_file.exists():
                with open(source_file, 'r') as f:
                    source_code = f.read()
                
                # Label: 1 if vulnerabilities found, 0 otherwise
                label = 1 if report.get('vulnerabilities', []) else 0
                
                entry = {
                    'source_code': source_code,
                    'label': label,
                    'vulnerabilities': report.get('vulnerabilities', []),
                    'cutoff_date': '2023-01-01'  # Update with actual fetch date
                }
                
                output_file = processed_dir / f"{contract_name}.json"
                with open(output_file, 'w') as f:
                    json.dump(entry, f, indent=2)
    
    # Compute hash for versioning
    dataset_hash = compute_dataset_hash(processed_dir)
    with open(OUTPUT_DIR / "dataset_version.txt", 'w') as f:
        f.write(f"Hash: {dataset_hash}\nDate: {json.dumps({'fetch_date': '2023-10-01'})}")
    
    print(f"Processed dataset saved to {processed_dir}")
    print(f"Version hash: {dataset_hash[:16]}...")


def process_vuldee_dataset():
    """Process local VulDeeSmartContract dataset from txt file."""
    if not VULDEE_DATA_PATH.exists():
        print("VulDeeSmartContract dataset not found.")
        return

    VULDEE_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
    
    current_entry = None
    current_code = []
    
    with open(VULDEE_DATA_PATH, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.rstrip('\n')
            if line.strip() == "---------------------------------":
                if current_entry:
                    source_code = '\n'.join(current_code) + '\n'
                    vulns = ['reentrancy'] if current_entry['label'] == 1 else []
                    entry = {
                        'source_code': source_code,
                        'label': current_entry['label'],
                        'vulnerabilities': vulns,
                        'cutoff_date': '2023-01-01'  # Approximate, update as needed
                    }
                    output_file = VULDEE_PROCESSED_DIR / f"{current_entry['filename']}.json"
                    with open(output_file, 'w', encoding='utf-8') as out_f:
                        json.dump(entry, out_f, indent=2)
                    current_entry = None
                    current_code = []
                continue
            
            parts = line.split(maxsplit=1)
            if len(parts) == 2 and parts[0].isdigit():
                if current_entry:
                    # Save previous
                    source_code = '\n'.join(current_code) + '\n'
                    vulns = ['reentrancy'] if current_entry['label'] == 1 else []
                    entry = {
                        'source_code': source_code,
                        'label': current_entry['label'],
                        'vulnerabilities': vulns,
                        'cutoff_date': '2023-01-01'
                    }
                    output_file = VULDEE_PROCESSED_DIR / f"{current_entry['filename']}.json"
                    with open(output_file, 'w', encoding='utf-8') as out_f:
                        json.dump(entry, out_f, indent=2)
                label = int(parts[0])
                filename = parts[1].strip()
                current_entry = {'label': label, 'filename': filename}
                current_code = []
            elif current_entry:
                current_code.append(line)
        
        # Handle last entry if any
        if current_entry:
            source_code = '\n'.join(current_code) + '\n'
            vulns = ['reentrancy'] if current_entry['label'] == 1 else []
            entry = {
                'source_code': source_code,
                'label': current_entry['label'],
                'vulnerabilities': vulns,
                'cutoff_date': '2023-01-01'
            }
            output_file = VULDEE_PROCESSED_DIR / f"{current_entry['filename']}.json"
            with open(output_file, 'w', encoding='utf-8') as out_f:
                json.dump(entry, out_f, indent=2)
    
    # Compute hash for versioning (reuse logic for processed_dir, but specify vuldee subdir if needed)
    dataset_hash = compute_dataset_hash(VULDEE_PROCESSED_DIR)
    with open(OUTPUT_DIR / "vuldee_dataset_version.txt", 'w') as f:
        f.write(f"Hash: {dataset_hash}\nDate: {json.dumps({'fetch_date': '2023-10-01'})}")
    
    print(f"VulDeeSmartContract processed dataset saved to {VULDEE_PROCESSED_DIR}")
    print(f"Version hash: {dataset_hash[:16]}...")


if __name__ == "__main__":
    fetch_smartbugs_dataset()
    preprocess_dataset()
    if VULDEE_DATA_PATH.exists():
        process_vuldee_dataset()
    print("Dataset preparation complete. Ready for ML fine-tuning.")