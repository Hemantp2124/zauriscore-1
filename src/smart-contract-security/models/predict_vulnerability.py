import os
import numpy as np
import joblib
from transformers import AutoTokenizer, AutoModel
import logging
import argparse
import torch
import requests
from dotenv import load_dotenv
import json
from typing import Dict, List, Optional, Union, Any, Tuple

__all__ = ['predict_vulnerabilities']

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration ---
MODELS_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "models"))
MODEL_FILE = os.path.join(MODELS_DIR, "codebert_vuln_classifier.joblib")
TOKENIZER_NAME = "microsoft/codebert-base"

# Global cache for tokenizer and model
tokenizer_global = None
model_global = None
device_global = None

def load_codebert_model_and_tokenizer():
    global tokenizer_global, model_global, device_global
    if tokenizer_global is None or model_global is None:
        try:
            logging.info(f"Loading CodeBERT tokenizer ({TOKENIZER_NAME})...")
            tokenizer_global = AutoTokenizer.from_pretrained(TOKENIZER_NAME)
            logging.info(f"Loading CodeBERT model ({TOKENIZER_NAME})...")
            model_global = AutoModel.from_pretrained(TOKENIZER_NAME)
            
            device_global = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model_global.to(device_global)
            logging.info(f"CodeBERT tokenizer and model loaded successfully on {device_global}.")
        except Exception as e:
            logging.error(f"Failed to load CodeBERT model or tokenizer: {e}")
            raise
    return tokenizer_global, model_global, device_global

def get_embedding(code_snippet, tokenizer, model, device):
    try:
        inputs = tokenizer(code_snippet, return_tensors="pt", truncation=True, max_length=512, padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()
        return embedding
    except Exception as e:
        logging.error(f"Error generating embedding: {e}\nSnippet (first 100 chars): {code_snippet[:100]}")
        return None

def fetch_code_from_etherscan(address, api_key):
    logging.info(f"Attempting to fetch source code for address: {address} from Etherscan...")
    api_url = f"https://api.etherscan.io/api?module=contract&action=getsourcecode&address={address}&apikey={api_key}"
    try:
        response = requests.get(api_url, timeout=30) # Added timeout
        response.raise_for_status()  # Raise an exception for HTTP errors
        data = response.json()

        if data["status"] == "1" and data["message"] == "OK":
            source_code_info = data["result"][0]
            if source_code_info.get("SourceCode"):
                source_code = source_code_info["SourceCode"]
                # Handle contracts that use the "Standard JSON Input" format
                if source_code.startswith("{") and source_code.endswith("}"):
                    try:
                        # If it's a JSON object, it might be a multi-file contract.
                        # We'll try to parse it and extract sources.
                        json_input = json.loads(source_code)
                        if 'sources' in json_input and isinstance(json_input['sources'], dict):
                            # Concatenate all source files. This is a simplification.
                            # A more advanced approach might analyze them separately or find the "main" contract.
                            all_sources = ""
                            for file_path, content_obj in json_input['sources'].items():
                                if 'content' in content_obj:
                                    all_sources += f"// File: {file_path}\n{content_obj['content']}\n\n"
                            if all_sources:
                                logging.info(f"Fetched and combined multiple source files for {address}.")
                                return all_sources.strip()
                            else:
                                logging.warning(f"Standard JSON input for {address} found, but no 'content' in sources.")
                                return None # Or handle as error
                        elif 'source' in json_input and isinstance(json_input['source'], dict): # Truffle-style?
                             all_sources = ""
                             for file_path, content_obj in json_input['source'].items():
                                if 'content' in content_obj:
                                    all_sources += f"// File: {file_path}\n{content_obj['content']}\n\n"
                             if all_sources:
                                logging.info(f"Fetched and combined multiple source files (Truffle-style) for {address}.")
                                return all_sources.strip()
                             else:
                                logging.warning(f"Standard JSON input (Truffle-style) for {address} found, but no 'content' in sources.")
                                return None
                        else: # If not 'sources', assume the whole string is the contract code (might be a JSON string itself as code)
                            logging.info(f"Fetched source code for {address} (possibly JSON string that is the code).")
                            return source_code

                    except json.JSONDecodeError:
                        # If it's not valid JSON, treat it as a single Solidity file
                        logging.info(f"Fetched source code for {address} (single file).")
                        return source_code
                else:
                    # Standard single file source code
                    logging.info(f"Fetched source code for {address} (single file).")
                    return source_code
            else:
                logging.warning(f"Contract {address} is not verified or source code is empty on Etherscan.")
                return None
        else:
            logging.error(f"Etherscan API error for {address}: {data.get('message', 'Unknown error')} - {data.get('result', '')}")
            return None
    except requests.exceptions.RequestException as e:
        logging.error(f"HTTP request failed when fetching code for {address}: {e}")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred while fetching code for {address}: {e}")
        return None


def analyze_code_content(code_content: str, source_name_for_log: str, classifier, tokenizer, model, device) -> Dict[str, Optional[float]]:
    """Analyze code content for vulnerabilities.
    
    Args:
        code_content (str): The source code to analyze
        source_name_for_log (str): Name of the source for logging
        classifier: The trained classifier model
        tokenizer: CodeBERT tokenizer
        model: CodeBERT model
        device: PyTorch device
        
    Returns:
        dict: Analysis results including vulnerability score
    """
    try:
        logging.info(f"Analyzing {source_name_for_log}...")
        
        # Get code embedding
        code_embedding = get_embedding(code_content, tokenizer, model, device)
        
        # Make prediction
        prediction = classifier.predict_proba([code_embedding])[0]
        vulnerability_score = prediction[1]  # Probability of being vulnerable
        
        logging.info(f"Analysis complete for {source_name_for_log}. Vulnerability score: {vulnerability_score:.4f}")
        
        return {
            'vulnerability_score': float(vulnerability_score),
            'source_name': source_name_for_log,
            'is_vulnerable': vulnerability_score > 0.5
        }
        
    except Exception as e:
        logging.error(f"Error analyzing {source_name_for_log}: {str(e)}")
        return None

def predict_vulnerabilities(code_content: str, source_name: str = 'Unknown') -> Dict[str, Optional[float]]:
    """Predict vulnerabilities in the given code.
    
    Args:
        code_content (str): The source code to analyze
        source_name (str): Name of the source for logging
        
    Returns:
        dict: Analysis results including vulnerability score
    """
    try:
        # Load model and tokenizer
        load_codebert_model_and_tokenizer()
        
        # Load classifier
        if not os.path.exists(MODEL_FILE):
            raise FileNotFoundError(f"Classifier model not found at {MODEL_FILE}")
            
        classifier = joblib.load(MODEL_FILE)
        
        # Run analysis
        return analyze_code_content(
            code_content,
            source_name,
            classifier,
            tokenizer_global,
            model_global,
            device_global
        )
        
    except Exception as e:
        logging.error(f"Error in vulnerability prediction: {str(e)}")
        return {
            'error': str(e),
            'vulnerability_score': None,
            'source_name': source_name
        }

def main():
    load_dotenv(os.path.join(os.path.dirname(__file__), "..", ".env")) # Load .env from project root
    api_key = os.getenv("ETHERSCAN_API_KEY")

    parser = argparse.ArgumentParser(description="Predict vulnerability of a Solidity contract via address, file, or direct code.")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--address", type=str, help="Ethereum contract address to analyze (fetches from Etherscan).")
    group.add_argument("--file", type=str, dest="contract_path", help="Path to the local .sol contract file.")
    group.add_argument("--code", type=str, help="Direct Solidity code string to analyze.")
    
    args = parser.parse_args()

    # Load CodeBERT model and tokenizer
    try:
        tokenizer, model, device = load_codebert_model_and_tokenizer()
    except Exception:
        return # Error already logged

    # Load the trained classifier
    if not os.path.exists(MODEL_FILE):
        logging.error(f"Trained model not found: {MODEL_FILE}")
        logging.error("Please ensure 'codebert_vuln_classifier.joblib' exists in the 'models' directory.")
        return
    
    try:
        classifier = joblib.load(MODEL_FILE)
        logging.info(f"Loaded trained classifier from {MODEL_FILE}")
    except Exception as e:
        logging.error(f"Error loading trained classifier: {e}")
        return

    code_to_analyze = None
    source_name_for_log = "Unknown_Source"

    if args.address:
        source_name_for_log = args.address
        if not api_key:
            logging.error("ETHERSCAN_API_KEY not found in .env file. Cannot fetch code for address.")
            logging.error("Please ensure your .env file is in the project root (zauriscore/.env) and contains ETHERSCAN_API_KEY.")
            return
        code_to_analyze = fetch_code_from_etherscan(args.address, api_key)
        if not code_to_analyze:
            # Error already logged by fetch_code_from_etherscan
            return
    elif args.contract_path:
        source_name_for_log = os.path.basename(args.contract_path)
        if not os.path.exists(args.contract_path):
            logging.error(f"Contract file not found: {args.contract_path}")
            return
        try:
            with open(args.contract_path, "r", encoding="utf-8", errors="ignore") as f:
                code_to_analyze = f.read()
            logging.info(f"Successfully read code from file: {args.contract_path}")
        except Exception as e:
            logging.error(f"Failed to read contract file {args.contract_path}: {e}")
            return
    elif args.code:
        source_name_for_log = "direct_code_input"
        code_to_analyze = args.code
        logging.info("Processing direct code input.")


    if not code_to_analyze or not code_to_analyze.strip():
        logging.error(f"No valid code provided or obtained for {source_name_for_log}. Analysis cannot proceed.")
        return

    analyze_code_content(code_to_analyze, source_name_for_log, classifier, tokenizer, model, device)

if __name__ == "__main__":
    main()
